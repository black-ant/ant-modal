"""
Whisper è¯­éŸ³è½¬æ–‡å­—æœåŠ¡
ä½¿ç”¨ OpenAI Whisper æ¨¡å‹è¿›è¡Œè¯­éŸ³è¯†åˆ«
"""
import modal

app = modal.App("whisper-stt")

# æ„å»ºé•œåƒ
image = (
    modal.Image.debian_slim(python_version="3.11")
    .apt_install("ffmpeg")  # Whisper éœ€è¦ ffmpeg
    .pip_install(
        "openai-whisper",
        "torch==2.1.0",
    )
)

# æ¨¡å‹ç¼“å­˜
model_volume = modal.Volume.from_name("whisper-models", create_if_missing=True)


@app.cls(
    image=image,
    gpu="T4",  # Whisper ä¸éœ€è¦å¤ªå¤§çš„ GPU
    volumes={"/models": model_volume},
    timeout=600,
)
class WhisperSTT:
    @modal.enter()
    def load_model(self):
        """åŠ è½½ Whisper æ¨¡å‹"""
        import whisper
        
        print("ğŸ¤ åŠ è½½ Whisper æ¨¡å‹...")
        
        # å¯é€‰: tiny, base, small, medium, large
        self.model = whisper.load_model(
            "medium",
            download_root="/models"
        )
        
        print("âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")
    
    @modal.method()
    def transcribe(
        self,
        audio_data: bytes,
        language: str = None,
        task: str = "transcribe"
    ) -> dict:
        """
        è¯­éŸ³è½¬æ–‡å­—
        
        Args:
            audio_data: éŸ³é¢‘æ–‡ä»¶å­—èŠ‚æ•°æ®
            language: è¯­è¨€ä»£ç  (zh, en, ja ç­‰)ï¼ŒNone ä¸ºè‡ªåŠ¨æ£€æµ‹
            task: "transcribe" æˆ– "translate" (ç¿»è¯‘æˆè‹±æ–‡)
        
        Returns:
            è½¬å½•ç»“æœ
        """
        import tempfile
        import os
        
        # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as f:
            f.write(audio_data)
            temp_path = f.name
        
        try:
            print(f"ğŸ¤ è½¬å½•éŸ³é¢‘...")
            
            result = self.model.transcribe(
                temp_path,
                language=language,
                task=task,
                fp16=True
            )
            
            print(f"âœ“ è½¬å½•å®Œæˆ")
            
            return {
                "text": result["text"],
                "language": result.get("language"),
                "segments": [
                    {
                        "start": seg["start"],
                        "end": seg["end"],
                        "text": seg["text"]
                    }
                    for seg in result.get("segments", [])
                ]
            }
        finally:
            os.unlink(temp_path)


@app.function(image=image)
@modal.web_endpoint(method="POST")
def transcribe_audio(audio: bytes, language: str = None):
    """
    Web API ç«¯ç‚¹
    
    POST /transcribe_audio
    Content-Type: audio/wav (æˆ–å…¶ä»–éŸ³é¢‘æ ¼å¼)
    
    Query params:
    - language: è¯­è¨€ä»£ç  (å¯é€‰)
    """
    whisper = WhisperSTT()
    result = whisper.transcribe.remote(audio, language=language)
    return result


@app.local_entrypoint()
def main(audio_file: str):
    """
    æœ¬åœ°æµ‹è¯•
    
    ä½¿ç”¨æ–¹æ³•:
    modal run whisper_service.py --audio-file=audio.mp3
    """
    whisper = WhisperSTT()
    
    with open(audio_file, "rb") as f:
        audio_data = f.read()
    
    result = whisper.transcribe.remote(audio_data)
    
    print(f"\nğŸ“ è½¬å½•ç»“æœ:\n{result['text']}\n")
    print(f"è¯­è¨€: {result['language']}")
    
    if result.get('segments'):
        print("\næ—¶é—´è½´:")
        for seg in result['segments'][:5]:  # åªæ˜¾ç¤ºå‰5æ®µ
            print(f"  [{seg['start']:.2f}s - {seg['end']:.2f}s] {seg['text']}")
