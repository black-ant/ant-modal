"""
ä¼šè®®çºªè¦è‡ªåŠ¨ç”Ÿæˆ
ä¸šåŠ¡åœºæ™¯ï¼šæ¯æ¬¡ä¼šè®®åéœ€è¦æ•´ç†ä¼šè®®çºªè¦ï¼Œè€—æ—¶ä¸”å®¹æ˜“é—æ¼

è§£å†³çš„é—®é¢˜ï¼š
- ä¸€å°æ—¶ä¼šè®®ï¼Œæ•´ç†çºªè¦éœ€è¦2å°æ—¶
- ç»å¸¸æ¼æ‰é‡è¦ä¿¡æ¯å’Œå¾…åŠäº‹é¡¹
- ä¸åŒäººæ•´ç†çš„æ ¼å¼ä¸ç»Ÿä¸€

è¿™ä¸ªä¾‹å­å±•ç¤ºï¼š
- ä¼šè®®å½•éŸ³è½¬æ–‡å­—
- è‡ªåŠ¨æå–å…³é”®ä¿¡æ¯å’Œå¾…åŠäº‹é¡¹
- ç”Ÿæˆç»“æ„åŒ–ä¼šè®®çºªè¦
- æŒ‰å‘è¨€äººåˆ†æ®µï¼ˆå¦‚æœæä¾›ï¼‰
"""
import modal
import json
from datetime import datetime
import re

app = modal.App("whisper-meeting-minutes")

image = (
    modal.Image.debian_slim(python_version="3.11")
    .apt_install("ffmpeg")
    .pip_install(
        "openai-whisper",
        "torch==2.1.0",
    )
)

model_volume = modal.Volume.from_name("whisper-models", create_if_missing=True)
output_volume = modal.Volume.from_name("meeting-minutes", create_if_missing=True)


@app.cls(
    image=image,
    gpu="T4",
    volumes={"/models": model_volume, "/output": output_volume},
    timeout=1800,  # ä¼šè®®å½•éŸ³å¯èƒ½è¾ƒé•¿
)
class MeetingTranscriber:
    @modal.enter()
    def load_model(self):
        import whisper
        
        print("ğŸ¤ åŠ è½½ Whisper æ¨¡å‹...")
        self.model = whisper.load_model("medium", download_root="/models")
        print("âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")
    
    @modal.method()
    def transcribe_meeting(
        self,
        audio_data: bytes,
        language: str = "zh",
        meeting_info: dict = None
    ) -> dict:
        """
        è½¬å½•ä¼šè®®å½•éŸ³
        
        Args:
            audio_data: éŸ³é¢‘æ•°æ®
            language: è¯­è¨€ä»£ç 
            meeting_info: ä¼šè®®ä¿¡æ¯ {"title": "...", "date": "...", "participants": [...]}
        """
        import tempfile
        import os
        
        if meeting_info is None:
            meeting_info = {}
        
        # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as f:
            f.write(audio_data)
            temp_path = f.name
        
        try:
            print("ğŸ¤ å¼€å§‹è½¬å½•ä¼šè®®å½•éŸ³...")
            
            result = self.model.transcribe(
                temp_path,
                language=language,
                task="transcribe",
                fp16=True
            )
            
            print("âœ“ è½¬å½•å®Œæˆ")
            
            # æ„å»ºè½¬å½•ç»“æœ
            transcript = {
                "full_text": result["text"],
                "segments": [
                    {
                        "start": seg["start"],
                        "end": seg["end"],
                        "text": seg["text"].strip()
                    }
                    for seg in result.get("segments", [])
                ],
                "duration_minutes": result.get("segments", [{}])[-1].get("end", 0) / 60 if result.get("segments") else 0
            }
            
            return transcript
            
        finally:
            os.unlink(temp_path)
    
    @modal.method()
    def extract_key_points(self, transcript_text: str) -> dict:
        """
        ä»è½¬å½•æ–‡æœ¬ä¸­æå–å…³é”®ä¿¡æ¯
        
        ä½¿ç”¨è§„åˆ™æå–ï¼ˆå¯ä»¥æ›¿æ¢ä¸º LLM æå–ï¼‰
        """
        key_points = {
            "decisions": [],      # å†³ç­–äº‹é¡¹
            "action_items": [],   # å¾…åŠäº‹é¡¹
            "questions": [],      # æå‡ºçš„é—®é¢˜
            "key_topics": [],     # å…³é”®è®®é¢˜
        }
        
        # ç®€å•çš„è§„åˆ™æå–ï¼ˆå®é™…åœºæ™¯å¯ç”¨ LLMï¼‰
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', transcript_text)
        
        decision_keywords = ["å†³å®š", "ç¡®å®š", "åŒæ„", "é€šè¿‡", "æ‰¹å‡†"]
        action_keywords = ["éœ€è¦", "è´Ÿè´£", "è·Ÿè¿›", "å®Œæˆ", "å¤„ç†", "å®‰æ’"]
        question_keywords = ["ï¼Ÿ", "æ€ä¹ˆ", "å¦‚ä½•", "æ˜¯å¦", "èƒ½ä¸èƒ½"]
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
                
            # æ£€æµ‹å†³ç­–
            if any(kw in sentence for kw in decision_keywords):
                key_points["decisions"].append(sentence)
            
            # æ£€æµ‹å¾…åŠ
            if any(kw in sentence for kw in action_keywords):
                key_points["action_items"].append(sentence)
            
            # æ£€æµ‹é—®é¢˜
            if any(kw in sentence for kw in question_keywords):
                key_points["questions"].append(sentence)
        
        # å»é‡å¹¶é™åˆ¶æ•°é‡
        for key in key_points:
            key_points[key] = list(set(key_points[key]))[:10]
        
        return key_points


@app.function(
    image=image,
    volumes={"/output": output_volume},
    timeout=3600
)
def generate_meeting_minutes(
    audio_data: bytes,
    meeting_info: dict = None,
    language: str = "zh"
) -> dict:
    """
    ç”Ÿæˆå®Œæ•´çš„ä¼šè®®çºªè¦
    
    Args:
        audio_data: ä¼šè®®å½•éŸ³
        meeting_info: ä¼šè®®ä¿¡æ¯
        language: è¯­è¨€
    """
    import os
    
    if meeting_info is None:
        meeting_info = {
            "title": "ä¼šè®®",
            "date": datetime.now().strftime("%Y-%m-%d"),
            "participants": []
        }
    
    transcriber = MeetingTranscriber()
    
    print("ğŸ“ ç”Ÿæˆä¼šè®®çºªè¦")
    print(f"   ä¼šè®®: {meeting_info.get('title', 'æœªå‘½åä¼šè®®')}")
    print(f"   æ—¥æœŸ: {meeting_info.get('date')}")
    
    # 1. è½¬å½•éŸ³é¢‘
    print("\n1ï¸âƒ£ è½¬å½•ä¼šè®®å½•éŸ³...")
    transcript = transcriber.transcribe_meeting.remote(
        audio_data, language, meeting_info
    )
    
    # 2. æå–å…³é”®ç‚¹
    print("2ï¸âƒ£ æå–å…³é”®ä¿¡æ¯...")
    key_points = transcriber.extract_key_points.remote(transcript["full_text"])
    
    # 3. ç”Ÿæˆä¼šè®®çºªè¦
    print("3ï¸âƒ£ ç”Ÿæˆç»“æ„åŒ–çºªè¦...")
    
    minutes = {
        "title": meeting_info.get("title", "ä¼šè®®çºªè¦"),
        "date": meeting_info.get("date"),
        "participants": meeting_info.get("participants", []),
        "duration_minutes": round(transcript["duration_minutes"], 1),
        "generated_at": datetime.now().isoformat(),
        
        "summary": {
            "total_segments": len(transcript["segments"]),
            "decisions_count": len(key_points["decisions"]),
            "action_items_count": len(key_points["action_items"]),
        },
        
        "content": {
            "full_transcript": transcript["full_text"],
            "timeline": transcript["segments"][:20],  # åªä¿ç•™å‰20æ®µç”¨äºé¢„è§ˆ
            "decisions": key_points["decisions"],
            "action_items": key_points["action_items"],
            "questions": key_points["questions"],
        }
    }
    
    # 4. ä¿å­˜ä¼šè®®çºªè¦
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    safe_title = re.sub(r'[^\w\-]', '_', meeting_info.get("title", "meeting"))[:30]
    output_path = f"/output/{safe_title}_{timestamp}.json"
    
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(minutes, f, ensure_ascii=False, indent=2)
    
    output_volume.commit()
    
    print(f"\nâœ… ä¼šè®®çºªè¦ç”Ÿæˆå®Œæˆ")
    print(f"   æ—¶é•¿: {minutes['duration_minutes']} åˆ†é’Ÿ")
    print(f"   å†³ç­–: {minutes['summary']['decisions_count']} é¡¹")
    print(f"   å¾…åŠ: {minutes['summary']['action_items_count']} é¡¹")
    print(f"   ä¿å­˜: {output_path}")
    
    return minutes


@app.function(image=image)
@modal.web_endpoint(method="POST")
def meeting_minutes_api(audio: bytes, title: str = "ä¼šè®®", participants: str = ""):
    """
    ä¼šè®®çºªè¦ API
    
    POST /meeting_minutes_api
    Content-Type: audio/wav
    Query params:
    - title: ä¼šè®®æ ‡é¢˜
    - participants: å‚ä¼šäººï¼ˆé€—å·åˆ†éš”ï¼‰
    """
    meeting_info = {
        "title": title,
        "date": datetime.now().strftime("%Y-%m-%d"),
        "participants": [p.strip() for p in participants.split(",") if p.strip()]
    }
    
    minutes = generate_meeting_minutes.remote(audio, meeting_info)
    
    return {
        "status": "success",
        "meeting_title": minutes["title"],
        "duration_minutes": minutes["duration_minutes"],
        "decisions": minutes["content"]["decisions"],
        "action_items": minutes["content"]["action_items"]
    }


@app.local_entrypoint()
def main():
    """æ¼”ç¤ºï¼ˆéœ€è¦æä¾›éŸ³é¢‘æ–‡ä»¶ï¼‰"""
    print("ğŸ“ ä¼šè®®çºªè¦è‡ªåŠ¨ç”Ÿæˆ")
    print("=" * 50)
    print("\nä½¿ç”¨æ–¹æ³•:")
    print("modal run whisper_meeting_minutes.py")
    print("\nç„¶åè°ƒç”¨ API:")
    print("curl -X POST -H 'Content-Type: audio/wav' \\")
    print("     --data-binary @meeting.wav \\")
    print("     'https://your-app--meeting-minutes-api.modal.run?title=å‘¨ä¼š'")
    print("\nğŸ’¡ æç¤º:")
    print("1. æ”¯æŒ mp3, wav, m4a ç­‰æ ¼å¼")
    print("2. ä¼šè®®çºªè¦ä¿å­˜åœ¨ meeting-minutes Volume")
    print("3. å¯å¯¹æ¥ LLM ç”Ÿæˆæ›´æ™ºèƒ½çš„æ‘˜è¦")

