"""
æ™ºèƒ½å®¢æœæœºå™¨äºº
ä¸šåŠ¡åœºæ™¯ï¼šç”µå•†/SaaS å¹³å°éœ€è¦ 24/7 å“åº”å®¢æˆ·å’¨è¯¢

è§£å†³çš„é—®é¢˜ï¼š
- äººå·¥å®¢æœæˆæœ¬é«˜ï¼Œæ¯æœˆæ”¯å‡ºæ•°åä¸‡
- å¤œé—´å’ŒèŠ‚å‡æ—¥æ— æ³•å“åº”ï¼Œæµå¤±æ½œåœ¨å®¢æˆ·
- é‡å¤é—®é¢˜å æ¯” 80%ï¼Œäººå·¥å›ç­”æ•ˆç‡ä½

è¿™ä¸ªä¾‹å­å±•ç¤ºï¼š
- åŸºäºä¼ä¸šçŸ¥è¯†åº“çš„é—®ç­”
- å¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡ç®¡ç†
- è‡ªåŠ¨è¯†åˆ«éœ€è¦è½¬äººå·¥çš„åœºæ™¯
- å¯¹è¯è®°å½•å­˜å‚¨ä¸åˆ†æ
"""
import modal
import json
from datetime import datetime

app = modal.App("llm-customer-service")

image = (
    modal.Image.debian_slim(python_version="3.11")
    .pip_install(
        "transformers==4.36.0",
        "torch==2.1.0",
        "accelerate",
        "bitsandbytes",
    )
)

model_volume = modal.Volume.from_name("llm-models", create_if_missing=True)
conversation_dict = modal.Dict.from_name("cs-conversations", create_if_missing=True)

# ä¼ä¸šçŸ¥è¯†åº“ï¼ˆå®é™…åœºæ™¯ä¸­ä»æ•°æ®åº“æˆ–å‘é‡åº“è·å–ï¼‰
KNOWLEDGE_BASE = {
    "é€€æ¬¾æ”¿ç­–": "æˆ‘ä»¬æ”¯æŒ7å¤©æ— ç†ç”±é€€æ¬¾ã€‚è¯·åœ¨è®¢å•è¯¦æƒ…é¡µæäº¤é€€æ¬¾ç”³è¯·ï¼Œå®¡æ ¸é€šè¿‡å3-5ä¸ªå·¥ä½œæ—¥åˆ°è´¦ã€‚",
    "å‘è´§æ—¶é—´": "è®¢å•æ”¯ä»˜æˆåŠŸåï¼Œæˆ‘ä»¬ä¼šåœ¨24å°æ—¶å†…å‘è´§ã€‚åè¿œåœ°åŒºå¯èƒ½éœ€è¦48å°æ—¶ã€‚",
    "è¿è´¹è§„åˆ™": "æ»¡99å…ƒåŒ…é‚®ï¼Œä¸æ»¡99å…ƒæ”¶å–10å…ƒè¿è´¹ã€‚åè¿œåœ°åŒºï¼ˆæ–°ç–†ã€è¥¿è—ã€é’æµ·ï¼‰éœ€é¢å¤–æ”¯ä»˜20å…ƒã€‚",
    "ä¼šå‘˜æƒç›Š": "ä¼šå‘˜å¯äº«å—ï¼š1. ä¸“å±9æŠ˜ä¼˜æƒ  2. æ¯æœˆ10å…ƒæ— é—¨æ§›åˆ¸ 3. ä¼˜å…ˆå‘è´§ 4. ä¸“å±å®¢æœé€šé“",
    "æ”¯ä»˜æ–¹å¼": "æ”¯æŒå¾®ä¿¡æ”¯ä»˜ã€æ”¯ä»˜å®ã€é“¶è¡Œå¡ã€èŠ±å‘—åˆ†æœŸç­‰å¤šç§æ”¯ä»˜æ–¹å¼ã€‚",
    "æ¢è´§æµç¨‹": "æ”¶åˆ°å•†å“7å¤©å†…ï¼Œå•†å“æœªä½¿ç”¨ä¸”åŒ…è£…å®Œå¥½ï¼Œå¯ç”³è¯·æ¢è´§ã€‚è¯·è”ç³»å®¢æœè·å–æ¢è´§åœ°å€ã€‚",
}

# éœ€è¦è½¬äººå·¥çš„å…³é”®è¯
ESCALATION_KEYWORDS = ["æŠ•è¯‰", "ç»ç†", "äººå·¥", "é€€æ¬¾å¤±è´¥", "éª—å­", "ä¸¾æŠ¥", "å·¥å•†", "å¾‹å¸ˆ"]


@app.cls(
    image=image,
    gpu="A100",
    volumes={"/models": model_volume},
    timeout=600,
    container_idle_timeout=300,
)
class CustomerServiceBot:
    @modal.enter()
    def load_model(self):
        from transformers import AutoModelForCausalLM, AutoTokenizer
        import torch
        
        print("ğŸ¤– åŠ è½½å®¢æœæ¨¡å‹...")
        
        model_name = "meta-llama/Meta-Llama-3-8B-Instruct"
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name, cache_dir="/models"
        )
        
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            cache_dir="/models",
            load_in_8bit=True,
        )
        
        print("âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def search_knowledge(self, query: str) -> str:
        """åœ¨çŸ¥è¯†åº“ä¸­æœç´¢ç›¸å…³ä¿¡æ¯"""
        query_lower = query.lower()
        
        # ç®€å•å…³é”®è¯åŒ¹é…ï¼ˆå®é™…åœºæ™¯ç”¨å‘é‡æœç´¢ï¼‰
        for topic, answer in KNOWLEDGE_BASE.items():
            if any(kw in query_lower for kw in topic.lower().split()):
                return f"ã€çŸ¥è¯†åº“ã€‘{topic}ï¼š{answer}"
        
        return ""
    
    def check_escalation(self, message: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦è½¬äººå·¥"""
        return any(kw in message for kw in ESCALATION_KEYWORDS)
    
    @modal.method()
    def chat(
        self,
        session_id: str,
        user_message: str,
        history: list[dict] = None
    ) -> dict:
        """
        å¤„ç†å®¢æœå¯¹è¯
        
        Args:
            session_id: ä¼šè¯ID
            user_message: ç”¨æˆ·æ¶ˆæ¯
            history: å¯¹è¯å†å²
        
        Returns:
            å›å¤å’ŒçŠ¶æ€ä¿¡æ¯
        """
        if history is None:
            history = []
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è½¬äººå·¥
        if self.check_escalation(user_message):
            return {
                "response": "éå¸¸æŠ±æ­‰ç»™æ‚¨å¸¦æ¥ä¸ä¾¿ï¼Œæˆ‘å·²ä¸ºæ‚¨è½¬æ¥äººå·¥å®¢æœï¼Œè¯·ç¨å€™...",
                "status": "escalated",
                "reason": "æ£€æµ‹åˆ°éœ€è¦äººå·¥å¤„ç†çš„å…³é”®è¯"
            }
        
        # æœç´¢çŸ¥è¯†åº“
        knowledge = self.search_knowledge(user_message)
        
        # æ„å»ºç³»ç»Ÿæç¤º
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šã€å‹å¥½çš„å®¢æœåŠ©æ‰‹ã€‚è¯·éµå¾ªä»¥ä¸‹è§„åˆ™ï¼š
1. å›ç­”è¦ç®€æ´æ˜äº†ï¼Œæ§åˆ¶åœ¨100å­—ä»¥å†…
2. å¯¹å®¢æˆ·ä¿æŒç¤¼è²Œå’Œè€å¿ƒ
3. å¦‚æœæœ‰ç›¸å…³çŸ¥è¯†åº“å†…å®¹ï¼Œä¼˜å…ˆä½¿ç”¨çŸ¥è¯†åº“å›ç­”
4. å¦‚æœæ— æ³•è§£ç­”ï¼Œè¯šå®å‘ŠçŸ¥å¹¶å»ºè®®è”ç³»äººå·¥å®¢æœ
5. ä¸è¦ç¼–é€ ä¿¡æ¯"""
        
        if knowledge:
            system_prompt += f"\n\nå‚è€ƒä¿¡æ¯ï¼š\n{knowledge}"
        
        # æ„å»ºæ¶ˆæ¯
        messages = [{"role": "system", "content": system_prompt}]
        messages.extend(history[-6:])  # ä¿ç•™æœ€è¿‘ 3 è½®å¯¹è¯
        messages.append({"role": "user", "content": user_message})
        
        # ç”Ÿæˆå›å¤
        prompt = self.tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
        
        outputs = self.model.generate(
            **inputs,
            max_new_tokens=200,
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
        )
        
        response = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        ).strip()
        
        return {
            "response": response,
            "status": "bot_replied",
            "knowledge_used": bool(knowledge)
        }


@app.function(image=image)
def save_conversation(session_id: str, message: dict):
    """ä¿å­˜å¯¹è¯è®°å½•"""
    history = conversation_dict.get(session_id, [])
    history.append({
        **message,
        "timestamp": datetime.now().isoformat()
    })
    conversation_dict[session_id] = history


@app.function(image=image)
@modal.web_endpoint(method="POST")
def customer_service_api(data: dict):
    """
    å®¢æœ API ç«¯ç‚¹
    
    POST /customer_service_api
    {
        "session_id": "user_12345",
        "message": "è¯·é—®æ€ä¹ˆé€€æ¬¾ï¼Ÿ",
        "history": [
            {"role": "user", "content": "..."},
            {"role": "assistant", "content": "..."}
        ]
    }
    """
    session_id = data.get("session_id", "anonymous")
    user_message = data.get("message", "")
    history = data.get("history", [])
    
    bot = CustomerServiceBot()
    result = bot.chat.remote(session_id, user_message, history)
    
    # ä¿å­˜å¯¹è¯
    save_conversation.remote(session_id, {
        "role": "user",
        "content": user_message
    })
    save_conversation.remote(session_id, {
        "role": "assistant",
        "content": result["response"],
        "status": result["status"]
    })
    
    return {
        "session_id": session_id,
        "reply": result["response"],
        "status": result["status"],
        "knowledge_used": result.get("knowledge_used", False)
    }


@app.local_entrypoint()
def main():
    """æ¨¡æ‹Ÿå®¢æœå¯¹è¯"""
    print("ğŸ¤– æ™ºèƒ½å®¢æœæœºå™¨äººæ¼”ç¤º")
    print("=" * 50)
    
    bot = CustomerServiceBot()
    
    # æµ‹è¯•å¯¹è¯
    test_conversations = [
        "ä½ å¥½ï¼Œè¯·é—®æ€ä¹ˆé€€æ¬¾ï¼Ÿ",
        "éœ€è¦å¤šä¹…åˆ°è´¦ï¼Ÿ",
        "è¿è´¹æ€ä¹ˆç®—ï¼Ÿ",
        "æˆ‘è¦æŠ•è¯‰ï¼",  # è§¦å‘è½¬äººå·¥
    ]
    
    history = []
    session_id = "test_session"
    
    for user_msg in test_conversations:
        print(f"\nğŸ‘¤ ç”¨æˆ·: {user_msg}")
        
        result = bot.chat.remote(session_id, user_msg, history)
        
        print(f"ğŸ¤– å®¢æœ: {result['response']}")
        print(f"   çŠ¶æ€: {result['status']}")
        
        if result["status"] == "escalated":
            print("   âš ï¸ å·²è½¬äººå·¥å®¢æœ")
            break
        
        # æ›´æ–°å†å²
        history.append({"role": "user", "content": user_msg})
        history.append({"role": "assistant", "content": result["response"]})
    
    print("\nğŸ’¡ æç¤º:")
    print("1. æ›´æ–° KNOWLEDGE_BASE æ·»åŠ ä¼ä¸šçŸ¥è¯†åº“")
    print("2. å¯¹æ¥å‘é‡æ•°æ®åº“å®ç°è¯­ä¹‰æœç´¢")
    print("3. é›†æˆåˆ°ç°æœ‰å®¢æœç³»ç»Ÿï¼ˆå¦‚ç½‘é¡µèŠå¤©ç»„ä»¶ï¼‰")

