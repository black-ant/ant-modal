"""
å†…å®¹å®¡æ ¸åŠ©æ‰‹
ä¸šåŠ¡åœºæ™¯ï¼šUGC å¹³å°éœ€è¦å¿«é€Ÿå®¡æ ¸å¤§é‡ç”¨æˆ·å‘å¸ƒçš„å†…å®¹

è§£å†³çš„é—®é¢˜ï¼š
- æ¯å¤©æ•°ä¸‡æ¡å†…å®¹ï¼Œäººå·¥å®¡æ ¸ä¸è¿‡æ¥
- è¿è§„å†…å®¹å½±å“å¹³å°å®‰å…¨å’Œåˆè§„
- å®¡æ ¸æ ‡å‡†ä¸ä¸€è‡´ï¼Œè´¨é‡éš¾ä»¥ä¿è¯

è¿™ä¸ªä¾‹å­å±•ç¤ºï¼š
- å¤šç»´åº¦å†…å®¹å®‰å…¨æ£€æµ‹
- æ‰¹é‡å¹¶è¡Œå®¡æ ¸æå‡æ•ˆç‡
- è‡ªåŠ¨åˆ†ç±»å’Œæ‰“æ ‡ç­¾
- å¯ç–‘å†…å®¹æ ‡è®°äººå·¥å¤å®¡
"""
import modal
import json
from datetime import datetime
from enum import Enum

app = modal.App("llm-content-review")

image = (
    modal.Image.debian_slim(python_version="3.11")
    .pip_install(
        "transformers==4.36.0",
        "torch==2.1.0",
        "accelerate",
        "bitsandbytes",
    )
)

model_volume = modal.Volume.from_name("llm-models", create_if_missing=True)

# å®¡æ ¸ç»´åº¦
REVIEW_DIMENSIONS = [
    "è¿æ³•è¿è§„",      # æ¶‰åŠè¿æ³•å†…å®¹
    "è‰²æƒ…ä½ä¿—",      # è‰²æƒ…ã€ä½ä¿—å†…å®¹
    "æš´åŠ›è¡€è…¥",      # æš´åŠ›ã€è¡€è…¥æè¿°
    "æ”¿æ²»æ•æ„Ÿ",      # æ”¿æ²»æ•æ„Ÿè¯é¢˜
    "å¹¿å‘Šè¥é”€",      # æœªç»æˆæƒçš„å¹¿å‘Š
    "è™šå‡ä¿¡æ¯",      # è°£è¨€ã€è™šå‡ä¿¡æ¯
    "äººèº«æ”»å‡»",      # ä¾®è¾±ã€è¯½è°¤ä»–äºº
    "éšç§æ³„éœ²",      # åŒ…å«ä¸ªäººéšç§ä¿¡æ¯
]


@app.cls(
    image=image,
    gpu="A100",
    volumes={"/models": model_volume},
    timeout=600,
    container_idle_timeout=300,
)
class ContentReviewer:
    @modal.enter()
    def load_model(self):
        from transformers import AutoModelForCausalLM, AutoTokenizer
        import torch
        
        print("ğŸ” åŠ è½½å®¡æ ¸æ¨¡å‹...")
        
        model_name = "meta-llama/Meta-Llama-3-8B-Instruct"
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name, cache_dir="/models"
        )
        
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            cache_dir="/models",
            load_in_8bit=True,
        )
        
        print("âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")
    
    @modal.method()
    def review_content(self, content: str, content_type: str = "text") -> dict:
        """
        å®¡æ ¸å•æ¡å†…å®¹
        
        Args:
            content: å¾…å®¡æ ¸å†…å®¹
            content_type: å†…å®¹ç±»å‹ (text/title/comment)
        
        Returns:
            å®¡æ ¸ç»“æœ
        """
        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªå†…å®¹å®‰å…¨å®¡æ ¸ä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹{content_type}å†…å®¹ï¼Œæ£€æŸ¥æ˜¯å¦å­˜åœ¨è¿è§„é—®é¢˜ã€‚

å®¡æ ¸ç»´åº¦ï¼š
{chr(10).join(f'- {dim}' for dim in REVIEW_DIMENSIONS)}

è¯·æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¿”å›ç»“æœï¼š
{{
    "passed": true/false,
    "risk_level": "safe/low/medium/high",
    "violations": ["è¿è§„ç±»å‹1", "è¿è§„ç±»å‹2"],
    "reason": "ç®€çŸ­è¯´æ˜",
    "suggestion": "å¤„ç†å»ºè®®"
}}

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"è¯·å®¡æ ¸ä»¥ä¸‹å†…å®¹ï¼š\n\n{content}"}
        ]
        
        prompt = self.tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
        
        outputs = self.model.generate(
            **inputs,
            max_new_tokens=300,
            temperature=0.1,  # ä½æ¸©åº¦ç¡®ä¿è¾“å‡ºç¨³å®š
            do_sample=True,
        )
        
        response = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        ).strip()
        
        # è§£æ JSON ç»“æœ
        try:
            # å°è¯•æå– JSON
            if "{" in response and "}" in response:
                json_str = response[response.find("{"):response.rfind("}")+1]
                result = json.loads(json_str)
            else:
                result = {
                    "passed": True,
                    "risk_level": "safe",
                    "violations": [],
                    "reason": "æ— æ˜æ˜¾è¿è§„",
                    "suggestion": "å¯ä»¥é€šè¿‡"
                }
        except json.JSONDecodeError:
            result = {
                "passed": True,
                "risk_level": "low",
                "violations": [],
                "reason": "è§£æå¼‚å¸¸ï¼Œé»˜è®¤é€šè¿‡",
                "suggestion": "å»ºè®®äººå·¥å¤å®¡"
            }
        
        result["content_preview"] = content[:50] + "..." if len(content) > 50 else content
        result["reviewed_at"] = datetime.now().isoformat()
        
        return result


@app.function(image=image, timeout=1200)
def batch_review(contents: list[dict]) -> dict:
    """
    æ‰¹é‡å®¡æ ¸å†…å®¹
    
    Args:
        contents: å†…å®¹åˆ—è¡¨ [{"id": "1", "content": "...", "type": "text"}]
    
    Returns:
        æ‰¹é‡å®¡æ ¸ç»“æœ
    """
    reviewer = ContentReviewer()
    
    results = {
        "total": len(contents),
        "passed": 0,
        "rejected": 0,
        "need_review": 0,
        "details": []
    }
    
    print(f"ğŸ” å¼€å§‹æ‰¹é‡å®¡æ ¸ {len(contents)} æ¡å†…å®¹")
    
    for i, item in enumerate(contents, 1):
        print(f"  å®¡æ ¸è¿›åº¦: {i}/{len(contents)}")
        
        review_result = reviewer.review_content.remote(
            content=item["content"],
            content_type=item.get("type", "text")
        )
        
        review_result["content_id"] = item.get("id", str(i))
        
        # ç»Ÿè®¡
        if review_result["passed"]:
            if review_result["risk_level"] in ["low", "medium"]:
                results["need_review"] += 1
            else:
                results["passed"] += 1
        else:
            results["rejected"] += 1
        
        results["details"].append(review_result)
    
    print(f"\nğŸ“Š å®¡æ ¸å®Œæˆ:")
    print(f"   é€šè¿‡: {results['passed']}")
    print(f"   æ‹’ç»: {results['rejected']}")
    print(f"   éœ€å¤å®¡: {results['need_review']}")
    
    return results


@app.function(image=image)
@modal.web_endpoint(method="POST")
def review_api(data: dict):
    """
    å†…å®¹å®¡æ ¸ API
    
    POST /review_api
    {
        "content": "è¦å®¡æ ¸çš„å†…å®¹",
        "type": "text",  // text/title/comment
        "id": "content_123"
    }
    
    æˆ–æ‰¹é‡å®¡æ ¸ï¼š
    {
        "batch": [
            {"id": "1", "content": "å†…å®¹1", "type": "text"},
            {"id": "2", "content": "å†…å®¹2", "type": "comment"}
        ]
    }
    """
    if "batch" in data:
        result = batch_review.remote(data["batch"])
        return {"status": "success", "batch_result": result}
    else:
        reviewer = ContentReviewer()
        result = reviewer.review_content.remote(
            content=data.get("content", ""),
            content_type=data.get("type", "text")
        )
        result["content_id"] = data.get("id")
        return {"status": "success", "result": result}


@app.local_entrypoint()
def main():
    """æ¼”ç¤ºå†…å®¹å®¡æ ¸"""
    print("ğŸ” å†…å®¹å®¡æ ¸åŠ©æ‰‹æ¼”ç¤º")
    print("=" * 50)
    
    # æµ‹è¯•å†…å®¹
    test_contents = [
        {"id": "1", "content": "ä»Šå¤©å¤©æ°”çœŸå¥½ï¼Œåˆ†äº«ä¸€ä¸‹æˆ‘çš„æ—©é¤ç…§ç‰‡~", "type": "text"},
        {"id": "2", "content": "è¿™ä¸ªäº§å“å¤ªåƒåœ¾äº†ï¼Œåƒä¸‡åˆ«ä¹°ï¼éª—å­å…¬å¸ï¼", "type": "comment"},
        {"id": "3", "content": "å…³æ³¨æˆ‘ï¼Œå…è´¹é¢†å–iPhone15ï¼åŠ å¾®ä¿¡xxx", "type": "text"},
        {"id": "4", "content": "åˆ†äº«ä¸€ä¸ªè¶…å®ç”¨çš„å­¦ä¹ æ–¹æ³•ï¼Œå¸®åŠ©æˆ‘æé«˜äº†æ•ˆç‡", "type": "text"},
    ]
    
    results = batch_review.remote(test_contents)
    
    print("\nğŸ“‹ è¯¦ç»†ç»“æœ:")
    for detail in results["details"]:
        status = "âœ… é€šè¿‡" if detail["passed"] else "âŒ æ‹’ç»"
        print(f"\n{status} [{detail['content_id']}] {detail['content_preview']}")
        print(f"   é£é™©ç­‰çº§: {detail['risk_level']}")
        if detail["violations"]:
            print(f"   è¿è§„ç±»å‹: {', '.join(detail['violations'])}")
        print(f"   è¯´æ˜: {detail['reason']}")
    
    print("\nğŸ’¡ æç¤º:")
    print("1. å¯æ ¹æ®ä¸šåŠ¡è°ƒæ•´ REVIEW_DIMENSIONS")
    print("2. å¯¹æ¥æ•°æ®åº“è®°å½•å®¡æ ¸ç»“æœ")
    print("3. medium/high é£é™©å»ºè®®äººå·¥å¤å®¡")

