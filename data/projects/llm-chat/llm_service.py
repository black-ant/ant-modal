"""
LLM å¯¹è¯æœåŠ¡
ä½¿ç”¨ Llama 3 æˆ–å…¶ä»–å¼€æºå¤§è¯­è¨€æ¨¡å‹æä¾›å¯¹è¯æœåŠ¡
"""
import modal

app = modal.App("llm-chat")

# æ„å»ºé•œåƒ
image = (
    modal.Image.debian_slim(python_version="3.11")
    .pip_install(
        "transformers==4.36.0",
        "torch==2.1.0",
        "accelerate",
        "bitsandbytes",  # ç”¨äºé‡åŒ–
    )
)

# æ¨¡å‹ç¼“å­˜
model_volume = modal.Volume.from_name("llm-models", create_if_missing=True)


@app.cls(
    image=image,
    gpu="A100",  # Llama 3 éœ€è¦è¾ƒå¤§æ˜¾å­˜
    volumes={"/models": model_volume},
    timeout=600,
    container_idle_timeout=300,
)
class LLMChat:
    @modal.enter()
    def load_model(self):
        """åŠ è½½ Llama 3 æ¨¡å‹"""
        from transformers import AutoModelForCausalLM, AutoTokenizer
        import torch
        
        print("ğŸ¤– åŠ è½½ Llama 3 æ¨¡å‹...")
        
        model_name = "meta-llama/Meta-Llama-3-8B-Instruct"
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            cache_dir="/models"
        )
        
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            cache_dir="/models",
            load_in_8bit=True,  # 8bit é‡åŒ–èŠ‚çœæ˜¾å­˜
        )
        
        print("âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")
    
    @modal.method()
    def chat(
        self,
        messages: list[dict],
        max_tokens: int = 512,
        temperature: float = 0.7,
        top_p: float = 0.9
    ) -> str:
        """
        å¯¹è¯ç”Ÿæˆ
        
        Args:
            messages: å¯¹è¯å†å² [{"role": "user", "content": "..."}]
            max_tokens: æœ€å¤§ç”Ÿæˆé•¿åº¦
            temperature: æ¸©åº¦å‚æ•°
            top_p: nucleus sampling å‚æ•°
        
        Returns:
            æ¨¡å‹å›å¤
        """
        # åº”ç”¨èŠå¤©æ¨¡æ¿
        prompt = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
        
        print(f"ğŸ’¬ ç”Ÿæˆå›å¤...")
        
        outputs = self.model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
            do_sample=True,
        )
        
        response = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        print(f"âœ“ å›å¤ç”Ÿæˆå®Œæˆ")
        return response.strip()


@app.function(image=image)
@modal.web_endpoint(method="POST")
def chat_completion(data: dict):
    """
    OpenAI å…¼å®¹çš„èŠå¤© API
    
    POST /chat_completion
    {
        "messages": [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Hello!"}
        ],
        "max_tokens": 512,
        "temperature": 0.7
    }
    """
    llm = LLMChat()
    
    response = llm.chat.remote(
        messages=data.get("messages", []),
        max_tokens=data.get("max_tokens", 512),
        temperature=data.get("temperature", 0.7),
        top_p=data.get("top_p", 0.9)
    )
    
    return {
        "choices": [{
            "message": {
                "role": "assistant",
                "content": response
            }
        }]
    }


@app.local_entrypoint()
def main(prompt: str = "ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½"):
    """
    æœ¬åœ°æµ‹è¯•
    
    ä½¿ç”¨æ–¹æ³•:
    modal run llm_service.py --prompt="ä½ çš„é—®é¢˜"
    """
    llm = LLMChat()
    
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ã€‚"},
        {"role": "user", "content": prompt}
    ]
    
    response = llm.chat.remote(messages=messages)
    print(f"\nğŸ¤– å›å¤:\n{response}\n")
