"""
Qwen é€šä¹‰åƒé—®å¯¹è¯æœåŠ¡
éƒ¨ç½²é˜¿é‡Œé€šä¹‰åƒé—®å¤§æ¨¡å‹ï¼Œæ”¯æŒå¯¹è¯å’Œæ–‡æœ¬ç”Ÿæˆ

é€‚ç”¨åœºæ™¯ï¼š
- éœ€è¦ä¸­æ–‡èƒ½åŠ›å¼ºçš„å¯¹è¯æœåŠ¡
- é˜¿é‡Œäº‘ç”Ÿæ€ç³»ç»Ÿé›†æˆ
- éœ€è¦é•¿ä¸Šä¸‹æ–‡æ”¯æŒ
"""
import modal

app = modal.App("qwen-chat")

image = (
    modal.Image.debian_slim(python_version="3.11")
    .pip_install(
        "transformers==4.40.0",
        "torch==2.1.0",
        "accelerate",
        "bitsandbytes",
        "tiktoken",
    )
)

model_volume = modal.Volume.from_name("qwen-models", create_if_missing=True)


@app.cls(
    image=image,
    gpu="A100",
    volumes={"/models": model_volume},
    timeout=600,
    container_idle_timeout=300,
)
class QwenChat:
    @modal.enter()
    def load_model(self):
        from transformers import AutoModelForCausalLM, AutoTokenizer
        import torch
        
        print("ğŸ¤– åŠ è½½ Qwen æ¨¡å‹...")
        
        # Qwen2-7B-Instruct æˆ– Qwen1.5-14B-Chat
        model_name = "Qwen/Qwen2-7B-Instruct"
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            cache_dir="/models",
            trust_remote_code=True
        )
        
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            cache_dir="/models",
            trust_remote_code=True,
            load_in_8bit=True,
        )
        
        print("âœ“ Qwen æ¨¡å‹åŠ è½½å®Œæˆ")
    
    @modal.method()
    def chat(
        self,
        messages: list[dict],
        max_tokens: int = 1024,
        temperature: float = 0.7,
        top_p: float = 0.9
    ) -> str:
        """
        å¯¹è¯ç”Ÿæˆ
        
        Args:
            messages: å¯¹è¯å†å² [{"role": "user", "content": "..."}]
            max_tokens: æœ€å¤§ç”Ÿæˆé•¿åº¦
            temperature: æ¸©åº¦å‚æ•°
            top_p: nucleus sampling å‚æ•°
        """
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)
        
        outputs = self.model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
            do_sample=True,
        )
        
        generated_ids = outputs[0][len(inputs.input_ids[0]):]
        response = self.tokenizer.decode(generated_ids, skip_special_tokens=True)
        
        return response.strip()


@app.function(image=image)
@modal.web_endpoint(method="POST")
def chat_api(data: dict):
    """
    OpenAI å…¼å®¹çš„èŠå¤© API
    
    POST /chat_api
    {
        "messages": [{"role": "user", "content": "ä½ å¥½"}],
        "max_tokens": 1024,
        "temperature": 0.7
    }
    """
    qwen = QwenChat()
    
    response = qwen.chat.remote(
        messages=data.get("messages", []),
        max_tokens=data.get("max_tokens", 1024),
        temperature=data.get("temperature", 0.7),
        top_p=data.get("top_p", 0.9)
    )
    
    return {
        "choices": [{
            "message": {"role": "assistant", "content": response}
        }],
        "model": "qwen2-7b-instruct"
    }


@app.local_entrypoint()
def main(prompt: str = "è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"):
    qwen = QwenChat()
    
    messages = [
        {"role": "system", "content": "ä½ æ˜¯é€šä¹‰åƒé—®ï¼Œä¸€ä¸ªç”±é˜¿é‡Œäº‘å¼€å‘çš„AIåŠ©æ‰‹ã€‚"},
        {"role": "user", "content": prompt}
    ]
    
    response = qwen.chat.remote(messages=messages)
    print(f"\nğŸ¤– Qwen: {response}\n")

