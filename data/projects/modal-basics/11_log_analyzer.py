"""
11 - æ—¥å¿—åˆ†æä¸å¼‚å¸¸æ£€æµ‹
ä¸šåŠ¡åœºæ™¯ï¼šæœåŠ¡å™¨äº§ç”Ÿæµ·é‡æ—¥å¿—ï¼Œäººå·¥æ’æŸ¥é—®é¢˜å¦‚å¤§æµ·æé’ˆ

è§£å†³çš„é—®é¢˜ï¼š
- æ¯å¤©äº§ç”Ÿ GB çº§åˆ«çš„æ—¥å¿—æ–‡ä»¶ï¼Œæ— æ³•äººå·¥æ£€æŸ¥
- æœåŠ¡å‡ºé—®é¢˜æ—¶éœ€è¦å¿«é€Ÿå®šä½é”™è¯¯æ—¥å¿—
- éœ€è¦ç»Ÿè®¡åˆ†ææ—¥å¿—ä¸­çš„å¼‚å¸¸æ¨¡å¼

è¿™ä¸ªä¾‹å­å±•ç¤ºï¼š
- å¹¶è¡Œå¤„ç†å¤§é‡æ—¥å¿—æ–‡ä»¶
- ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…å¼‚å¸¸æ¨¡å¼
- ç»Ÿè®¡æ±‡æ€»åˆ†æç»“æœ
- Volume å­˜å‚¨åˆ†ææŠ¥å‘Š
"""
import modal
import re
from datetime import datetime
from collections import Counter
import json

app = modal.App("log-analyzer")

# å­˜å‚¨åˆ†ææŠ¥å‘Š
volume = modal.Volume.from_name("log-reports", create_if_missing=True)

# å¼‚å¸¸æ¨¡å¼å®šä¹‰
ERROR_PATTERNS = [
    {"name": "ERROR çº§åˆ«", "pattern": r"\bERROR\b", "severity": "high"},
    {"name": "å¼‚å¸¸å †æ ˆ", "pattern": r"Exception|Traceback", "severity": "high"},
    {"name": "è¶…æ—¶é”™è¯¯", "pattern": r"timeout|timed?\s*out", "severity": "medium"},
    {"name": "è¿æ¥å¤±è´¥", "pattern": r"connection\s*(refused|reset|failed)", "severity": "medium"},
    {"name": "å†…å­˜é—®é¢˜", "pattern": r"out\s*of\s*memory|OOM|memory\s*error", "severity": "critical"},
    {"name": "ç£ç›˜é—®é¢˜", "pattern": r"disk\s*(full|space)|no\s*space\s*left", "severity": "critical"},
    {"name": "è®¤è¯å¤±è´¥", "pattern": r"authentication\s*failed|unauthorized|403|401", "severity": "medium"},
    {"name": "æ•°æ®åº“é”™è¯¯", "pattern": r"database\s*error|sql\s*error|deadlock", "severity": "high"},
]


@app.function()
def analyze_log_chunk(log_lines: list[str], chunk_id: int) -> dict:
    """
    åˆ†æä¸€å—æ—¥å¿—æ•°æ®
    åœ¨äº‘ç«¯å¹¶è¡Œå¤„ç†ï¼Œæ¯ä¸ªå—ç‹¬ç«‹åˆ†æ
    """
    result = {
        "chunk_id": chunk_id,
        "total_lines": len(log_lines),
        "error_counts": Counter(),
        "severity_counts": {"critical": 0, "high": 0, "medium": 0, "low": 0},
        "error_samples": [],  # ä¿å­˜ä¸€äº›é”™è¯¯æ ·æœ¬
        "hourly_distribution": Counter(),
    }
    
    # æ—¶é—´æˆ³æ­£åˆ™ï¼ˆå¸¸è§æ—¥å¿—æ ¼å¼ï¼‰
    timestamp_pattern = re.compile(r'(\d{4}-\d{2}-\d{2}\s+\d{2}):\d{2}:\d{2}')
    
    for line in log_lines:
        # æå–å°æ—¶åˆ†å¸ƒ
        ts_match = timestamp_pattern.search(line)
        if ts_match:
            hour = ts_match.group(1)
            result["hourly_distribution"][hour] += 1
        
        # æ£€æŸ¥å„ç§é”™è¯¯æ¨¡å¼
        for pattern_info in ERROR_PATTERNS:
            if re.search(pattern_info["pattern"], line, re.IGNORECASE):
                result["error_counts"][pattern_info["name"]] += 1
                result["severity_counts"][pattern_info["severity"]] += 1
                
                # ä¿å­˜å‰ 5 ä¸ªé”™è¯¯æ ·æœ¬
                if len(result["error_samples"]) < 5:
                    result["error_samples"].append({
                        "type": pattern_info["name"],
                        "severity": pattern_info["severity"],
                        "line": line[:200]  # æˆªæ–­è¿‡é•¿çš„è¡Œ
                    })
    
    return result


@app.function()
def merge_analysis_results(results: list[dict]) -> dict:
    """
    åˆå¹¶æ‰€æœ‰å—çš„åˆ†æç»“æœ
    """
    merged = {
        "total_lines": 0,
        "total_chunks": len(results),
        "error_counts": Counter(),
        "severity_counts": {"critical": 0, "high": 0, "medium": 0, "low": 0},
        "error_samples": [],
        "hourly_distribution": Counter(),
    }
    
    for result in results:
        merged["total_lines"] += result["total_lines"]
        
        for error_type, count in result["error_counts"].items():
            merged["error_counts"][error_type] += count
        
        for severity, count in result["severity_counts"].items():
            merged["severity_counts"][severity] += count
        
        merged["error_samples"].extend(result["error_samples"])
        
        for hour, count in result["hourly_distribution"].items():
            merged["hourly_distribution"][hour] += count
    
    # åªä¿ç•™å‰ 10 ä¸ªé”™è¯¯æ ·æœ¬
    merged["error_samples"] = merged["error_samples"][:10]
    
    # è½¬æ¢ Counter ä¸ºæ™®é€š dict ä»¥ä¾¿ JSON åºåˆ—åŒ–
    merged["error_counts"] = dict(merged["error_counts"])
    merged["hourly_distribution"] = dict(merged["hourly_distribution"])
    
    return merged


@app.function(volumes={"/reports": volume})
def save_analysis_report(analysis: dict, report_name: str) -> str:
    """
    ä¿å­˜åˆ†ææŠ¥å‘Š
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_path = f"/reports/{report_name}_{timestamp}.json"
    
    report = {
        "generated_at": datetime.now().isoformat(),
        "summary": {
            "total_lines_analyzed": analysis["total_lines"],
            "total_errors_found": sum(analysis["error_counts"].values()),
            "critical_issues": analysis["severity_counts"]["critical"],
            "high_issues": analysis["severity_counts"]["high"],
        },
        "details": analysis
    }
    
    with open(report_path, "w", encoding="utf-8") as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    volume.commit()
    return report_path


def generate_mock_logs(num_lines: int = 10000) -> list[str]:
    """
    ç”Ÿæˆæ¨¡æ‹Ÿæ—¥å¿—æ•°æ®ï¼ˆç”¨äºæ¼”ç¤ºï¼‰
    å®é™…åœºæ™¯ä¸­ï¼Œæ—¥å¿—ä¼šä»æ–‡ä»¶æˆ–æ—¥å¿—æœåŠ¡è¯»å–
    """
    import random
    
    log_templates = [
        "{ts} INFO  [main] Application started successfully",
        "{ts} DEBUG [worker-{n}] Processing request {n}",
        "{ts} INFO  [api] Request completed in {n}ms",
        "{ts} WARN  [db] Query took {n}ms, consider optimization",
        "{ts} ERROR [api] Request failed: Connection timeout",
        "{ts} ERROR [worker-{n}] Exception in thread: NullPointerException",
        "{ts} ERROR [db] Database connection failed: Connection refused",
        "{ts} ERROR [auth] Authentication failed for user_{n}",
        "{ts} CRITICAL [system] Out of memory error detected",
        "{ts} ERROR [api] Traceback (most recent call last):",
    ]
    
    logs = []
    base_time = datetime(2024, 1, 15, 0, 0, 0)
    
    for i in range(num_lines):
        # ç”Ÿæˆæ—¶é—´æˆ³
        ts = base_time.replace(
            hour=i % 24,
            minute=random.randint(0, 59),
            second=random.randint(0, 59)
        )
        ts_str = ts.strftime("%Y-%m-%d %H:%M:%S")
        
        # å¤§éƒ¨åˆ†æ˜¯æ­£å¸¸æ—¥å¿—ï¼Œå°‘éƒ¨åˆ†æ˜¯é”™è¯¯
        if random.random() < 0.05:  # 5% é”™è¯¯ç‡
            template = random.choice(log_templates[4:])  # é”™è¯¯æ¨¡æ¿
        else:
            template = random.choice(log_templates[:4])  # æ­£å¸¸æ¨¡æ¿
        
        log_line = template.format(ts=ts_str, n=random.randint(1, 100))
        logs.append(log_line)
    
    return logs


@app.local_entrypoint()
def main():
    """
    è¿è¡Œæ—¥å¿—åˆ†æ
    
    ä½¿ç”¨æ–¹æ³•ï¼š
    - è¿è¡Œåˆ†æï¼šmodal run 11_log_analyzer.py
    
    å®é™…ä½¿ç”¨ï¼š
    - ä» S3/Volume è¯»å–æ—¥å¿—æ–‡ä»¶
    - æˆ–ä»æ—¥å¿—æœåŠ¡ API è·å–æ—¥å¿—
    """
    print("ğŸ“‹ æ—¥å¿—åˆ†æä¸å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿ")
    print("=" * 50)
    
    # ç”Ÿæˆæ¨¡æ‹Ÿæ—¥å¿—
    print("ğŸ“ ç”Ÿæˆæ¨¡æ‹Ÿæ—¥å¿—æ•°æ®...")
    logs = generate_mock_logs(50000)  # 5 ä¸‡è¡Œæ—¥å¿—
    print(f"ğŸ“Š å…± {len(logs)} è¡Œæ—¥å¿—å¾…åˆ†æ")
    
    # åˆ†å—å¤„ç†
    chunk_size = 5000
    chunks = [logs[i:i+chunk_size] for i in range(0, len(logs), chunk_size)]
    print(f"ğŸ“¦ åˆ†æˆ {len(chunks)} å—å¹¶è¡Œå¤„ç†\n")
    
    # å¹¶è¡Œåˆ†ææ‰€æœ‰å—
    print("ğŸ” å¼€å§‹å¹¶è¡Œåˆ†æ...")
    chunk_results = list(analyze_log_chunk.starmap(
        [(chunk, i) for i, chunk in enumerate(chunks)]
    ))
    
    # åˆå¹¶ç»“æœ
    print("ğŸ“Š åˆå¹¶åˆ†æç»“æœ...")
    final_analysis = merge_analysis_results.remote(chunk_results)
    
    # æ‰“å°åˆ†æç»“æœ
    print("\n" + "=" * 50)
    print("ğŸ“ˆ åˆ†æç»“æœæ±‡æ€»")
    print("=" * 50)
    print(f"æ€»è¡Œæ•°: {final_analysis['total_lines']:,}")
    print(f"æ€»é”™è¯¯: {sum(final_analysis['error_counts'].values()):,}")
    print(f"\nğŸš¨ ä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ:")
    print(f"  - ä¸¥é‡ (Critical): {final_analysis['severity_counts']['critical']}")
    print(f"  - é«˜å± (High): {final_analysis['severity_counts']['high']}")
    print(f"  - ä¸­ç­‰ (Medium): {final_analysis['severity_counts']['medium']}")
    
    print(f"\nğŸ“Š é”™è¯¯ç±»å‹ç»Ÿè®¡:")
    for error_type, count in sorted(final_analysis['error_counts'].items(), 
                                     key=lambda x: x[1], reverse=True):
        print(f"  - {error_type}: {count}")
    
    if final_analysis['error_samples']:
        print(f"\nğŸ“ é”™è¯¯æ ·æœ¬:")
        for i, sample in enumerate(final_analysis['error_samples'][:3], 1):
            print(f"  {i}. [{sample['severity']}] {sample['type']}")
            print(f"     {sample['line'][:80]}...")
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = save_analysis_report.remote(final_analysis, "daily_log_analysis")
    print(f"\nğŸ’¾ æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    print("\nğŸ’¡ æç¤º:")
    print("1. ä¿®æ”¹ ERROR_PATTERNS æ·»åŠ è‡ªå®šä¹‰é”™è¯¯æ¨¡å¼")
    print("2. å®é™…ä½¿ç”¨æ—¶ä»æ—¥å¿—æ–‡ä»¶æˆ–æ—¥å¿—æœåŠ¡è¯»å–æ•°æ®")
    print("3. å¯ä»¥é…åˆå®šæ—¶ä»»åŠ¡å®ç°æ¯æ—¥è‡ªåŠ¨åˆ†æ")

