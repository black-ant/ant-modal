"""
Yi é›¶ä¸€ä¸‡ç‰©å¯¹è¯æœåŠ¡
éƒ¨ç½²é›¶ä¸€ä¸‡ç‰© Yi ç³»åˆ—å¤§æ¨¡åž‹

é€‚ç”¨åœºæ™¯ï¼š
- éœ€è¦åŒè¯­ï¼ˆä¸­è‹±æ–‡ï¼‰èƒ½åŠ›
- é•¿ä¸Šä¸‹æ–‡åœºæ™¯ï¼ˆYi æ”¯æŒ 200K ä¸Šä¸‹æ–‡ï¼‰
- é«˜è´¨é‡æ–‡æœ¬ç”Ÿæˆ
"""
import modal

app = modal.App("yi-chat")

image = (
    modal.Image.debian_slim(python_version="3.11")
    .pip_install(
        "transformers==4.40.0",
        "torch==2.1.0",
        "accelerate",
        "bitsandbytes",
        "sentencepiece",
    )
)

model_volume = modal.Volume.from_name("yi-models", create_if_missing=True)


@app.cls(
    image=image,
    gpu="A100",
    volumes={"/models": model_volume},
    timeout=600,
    container_idle_timeout=300,
)
class YiChat:
    @modal.enter()
    def load_model(self):
        from transformers import AutoModelForCausalLM, AutoTokenizer
        import torch
        
        print("ðŸ¤– åŠ è½½ Yi æ¨¡åž‹...")
        model_name = "01-ai/Yi-1.5-9B-Chat"
        
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir="/models", trust_remote_code=True)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name, torch_dtype=torch.float16, device_map="auto", cache_dir="/models", trust_remote_code=True, load_in_8bit=True
        )
        print("âœ“ Yi æ¨¡åž‹åŠ è½½å®Œæˆ")
    
    @modal.method()
    def chat(self, messages: list[dict], max_tokens: int = 1024, temperature: float = 0.7) -> str:
        input_ids = self.tokenizer.apply_chat_template(
            messages, tokenize=True, add_generation_prompt=True, return_tensors="pt"
        ).to(self.model.device)
        
        outputs = self.model.generate(
            input_ids, max_new_tokens=max_tokens, temperature=temperature, do_sample=True, eos_token_id=self.tokenizer.eos_token_id
        )
        return self.tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True).strip()


@app.function(image=image)
@modal.web_endpoint(method="POST")
def chat_api(data: dict):
    yi = YiChat()
    response = yi.chat.remote(messages=data.get("messages", []), max_tokens=data.get("max_tokens", 1024))
    return {"choices": [{"message": {"role": "assistant", "content": response}}], "model": "yi-1.5-9b"}


@app.local_entrypoint()
def main(prompt: str = "ä½ å¥½"):
    yi = YiChat()
    response = yi.chat.remote(messages=[{"role": "user", "content": prompt}])
    print(f"\nðŸ¤– Yi: {response}\n")

