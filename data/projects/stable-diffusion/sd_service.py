"""
Stable Diffusion å›¾åƒç”ŸæˆæœåŠ¡
ä½¿ç”¨ SDXL æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡å›¾åƒ
"""
import modal

app = modal.App("stable-diffusion")

# æ„å»ºåŒ…å« Stable Diffusion çš„é•œåƒ
image = (
    modal.Image.debian_slim(python_version="3.11")
    .pip_install(
        "diffusers==0.24.0",
        "transformers==4.36.0",
        "accelerate",
        "safetensors",
        "torch==2.1.0",
        "torchvision",
    )
)

# æ¨¡å‹ç¼“å­˜ Volume
model_volume = modal.Volume.from_name("sd-models", create_if_missing=True)


@app.cls(
    image=image,
    gpu="A10G",
    volumes={"/models": model_volume},
    timeout=600,
)
class StableDiffusion:
    @modal.enter()
    def load_model(self):
        """åŠ è½½ SDXL æ¨¡å‹"""
        from diffusers import DiffusionPipeline
        import torch
        
        print("ğŸ¨ åŠ è½½ Stable Diffusion XL æ¨¡å‹...")
        
        self.pipe = DiffusionPipeline.from_pretrained(
            "stabilityai/stable-diffusion-xl-base-1.0",
            torch_dtype=torch.float16,
            use_safetensors=True,
            variant="fp16",
            cache_dir="/models"
        )
        self.pipe.to("cuda")
        
        print("âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")
    
    @modal.method()
    def generate(
        self,
        prompt: str,
        negative_prompt: str = "",
        width: int = 1024,
        height: int = 1024,
        num_inference_steps: int = 30,
        guidance_scale: float = 7.5,
        seed: int = None
    ) -> bytes:
        """
        ç”Ÿæˆå›¾åƒ
        
        Args:
            prompt: æ­£å‘æç¤ºè¯
            negative_prompt: è´Ÿå‘æç¤ºè¯
            width: å›¾åƒå®½åº¦
            height: å›¾åƒé«˜åº¦
            num_inference_steps: æ¨ç†æ­¥æ•°
            guidance_scale: å¼•å¯¼ç³»æ•°
            seed: éšæœºç§å­
        
        Returns:
            å›¾åƒçš„å­—èŠ‚æ•°æ®
        """
        import torch
        import io
        
        if seed is not None:
            generator = torch.Generator("cuda").manual_seed(seed)
        else:
            generator = None
        
        print(f"ğŸ¨ ç”Ÿæˆå›¾åƒ: {prompt[:50]}...")
        
        image = self.pipe(
            prompt=prompt,
            negative_prompt=negative_prompt,
            width=width,
            height=height,
            num_inference_steps=num_inference_steps,
            guidance_scale=guidance_scale,
            generator=generator
        ).images[0]
        
        # è½¬æ¢ä¸ºå­—èŠ‚
        buf = io.BytesIO()
        image.save(buf, format="PNG")
        
        print("âœ“ å›¾åƒç”Ÿæˆå®Œæˆ")
        return buf.getvalue()


@app.function(image=image)
@modal.web_endpoint(method="POST")
def generate_image(data: dict):
    """
    Web API ç«¯ç‚¹
    
    POST /generate_image
    {
        "prompt": "a beautiful sunset over mountains",
        "negative_prompt": "blurry, low quality",
        "width": 1024,
        "height": 1024,
        "steps": 30,
        "guidance": 7.5,
        "seed": 42
    }
    """
    sd = StableDiffusion()
    
    image_bytes = sd.generate.remote(
        prompt=data.get("prompt", ""),
        negative_prompt=data.get("negative_prompt", ""),
        width=data.get("width", 1024),
        height=data.get("height", 1024),
        num_inference_steps=data.get("steps", 30),
        guidance_scale=data.get("guidance", 7.5),
        seed=data.get("seed")
    )
    
    import base64
    return {
        "image": base64.b64encode(image_bytes).decode(),
        "format": "png"
    }


@app.local_entrypoint()
def main(prompt: str = "a beautiful sunset over mountains"):
    """
    æœ¬åœ°æµ‹è¯•
    
    ä½¿ç”¨æ–¹æ³•:
    modal run sd_service.py --prompt="your prompt here"
    """
    sd = StableDiffusion()
    image_bytes = sd.generate.remote(prompt=prompt)
    
    # ä¿å­˜å›¾åƒ
    with open("output.png", "wb") as f:
        f.write(image_bytes)
    
    print("âœ“ å›¾åƒå·²ä¿å­˜åˆ° output.png")
