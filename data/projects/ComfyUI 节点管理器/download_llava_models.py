#!/usr/bin/env python3
"""
=============================================================================
LLAVA æ¨¡å‹ä¸‹è½½
=============================================================================
åŠŸèƒ½è¯´æ˜ï¼š
- ä» HuggingFace ä¸‹è½½ LLAVA GGUF æ¨¡å‹æ–‡ä»¶
  * Llama-Joycaption-Beta-One-Hf-Llava-F16.gguf
  * llama-joycaption-beta-one-llava-mmproj-model-f16.gguf
- åˆ›å»ºä¸“ç”¨çš„ llava_gguf ç›®å½•
- ç”¨äºå›¾åƒæè¿°å’Œç†è§£åŠŸèƒ½

ç¯å¢ƒè¦æ±‚ï¼š
    æ— ç‰¹æ®Šè¦æ±‚ï¼Œå…¬å¼€æ¨¡å‹

ä½¿ç”¨æ–¹æ³•ï¼š
    modal run download_llava_models.py

ç‹¬ç«‹è¿è¡Œï¼š
    æ­¤è„šæœ¬å¯ç‹¬ç«‹è¿è¡Œï¼Œä¸“æ³¨äº LLAVA å›¾åƒç†è§£æ¨¡å‹
=============================================================================
"""

import os
import subprocess
import modal

# =============================================================================
# S1: é…ç½®åŸºç¡€ç¯å¢ƒ
# =============================================================================

print("ğŸ”§ é…ç½® LLAVA æ¨¡å‹ä¸‹è½½ç¯å¢ƒ...")

# åŸºç¡€é•œåƒ
image = (
    modal.Image.debian_slim(python_version="3.11")
    .apt_install("git")
    .pip_install("comfy-cli==1.5.1")
    .run_commands("comfy --skip-prompt install --fast-deps --nvidia --version 0.3.59")
    .pip_install("huggingface_hub[hf_transfer]==0.34.4")
    .env({"HF_HUB_ENABLE_HF_TRANSFER": "1"})
)

# Volume æŒä¹…åŒ–å­˜å‚¨
vol = modal.Volume.from_name("hf-hub-cache", create_if_missing=True)

# =============================================================================
# S2: LLAVA æ¨¡å‹ä¸‹è½½å‡½æ•°
# =============================================================================

def download_llava_models():
    """ä¸‹è½½ LLAVA GGUF æ¨¡å‹"""
    from huggingface_hub import hf_hub_download
    
    print("ğŸ“¥ å¼€å§‹ä¸‹è½½ LLAVA GGUF æ¨¡å‹...")
    
    # LLAVA æ¨¡å‹åˆ—è¡¨
    llava_gguf_models = [
        {
            "repo_id": "concedo/llama-joycaption-beta-one-hf-llava-mmproj-gguf",
            "filename": "Llama-Joycaption-Beta-One-Hf-Llava-F16.gguf",
            "local_name": "Llama-Joycaption-Beta-One-Hf-Llava-F16.gguf",
            "description": "LLAVA ä¸»æ¨¡å‹"
        },
        {
            "repo_id": "concedo/llama-joycaption-beta-one-hf-llava-mmproj-gguf",
            "filename": "llama-joycaption-beta-one-llava-mmproj-model-f16.gguf",
            "local_name": "llama-joycaption-beta-one-llava-mmproj-model-f16.gguf",
            "description": "LLAVA æŠ•å½±æ¨¡å‹"
        }
    ]
    
    # åˆ›å»º llava_gguf ç›®å½•
    llava_gguf_dir = "/root/comfy/ComfyUI/models/llava_gguf"
    os.makedirs(llava_gguf_dir, exist_ok=True)
    
    # ä¸‹è½½æ¯ä¸ª LLAVA æ¨¡å‹
    for llava_model in llava_gguf_models:
        try:
            print(f"\nğŸ“¦ ä¸‹è½½ {llava_model['description']}: {llava_model['filename']}")
            llava_path = hf_hub_download(
                repo_id=llava_model["repo_id"],
                filename=llava_model["filename"],
                cache_dir="/cache",
            )
            
            # åˆ›å»ºè½¯é“¾æ¥
            target_path = f"{llava_gguf_dir}/{llava_model['local_name']}"
            subprocess.run(
                f"ln -sf {llava_path} {target_path}",
                shell=True,
                check=True
            )
            print(f"   âœ… {llava_model['local_name']} ä¸‹è½½å®Œæˆ")
            
        except Exception as e:
            print(f"   âŒ {llava_model['local_name']} ä¸‹è½½å¤±è´¥: {e}")
    
    print("\nâœ… LLAVA æ¨¡å‹ä¸‹è½½å®Œæˆï¼")


# æ„å»ºåŒ…å« LLAVA æ¨¡å‹çš„é•œåƒ
image = image.run_function(
    download_llava_models,
    volumes={"/cache": vol}
)

# =============================================================================
# S3: åˆ›å»º Modal åº”ç”¨
# =============================================================================

app = modal.App(name="comfyui-llava-models", image=image)

print("âœ… LLAVA æ¨¡å‹é•œåƒæ„å»ºå®Œæˆï¼")
print("ğŸ’¡ æç¤ºï¼šLLAVA æ¨¡å‹ç”¨äºå›¾åƒæè¿°å’Œç†è§£åŠŸèƒ½")


@app.function(volumes={"/cache": vol})
def verify_llava_models():
    """éªŒè¯ LLAVA æ¨¡å‹æ˜¯å¦ä¸‹è½½æˆåŠŸ"""
    import os
    from pathlib import Path
    
    llava_dir = Path("/root/comfy/ComfyUI/models/llava_gguf")
    
    expected_models = [
        "Llama-Joycaption-Beta-One-Hf-Llava-F16.gguf",
        "llama-joycaption-beta-one-llava-mmproj-model-f16.gguf"
    ]
    
    print("ğŸ” éªŒè¯ LLAVA æ¨¡å‹æ–‡ä»¶...")
    all_exist = True
    
    if llava_dir.exists():
        for model_name in expected_models:
            model_path = llava_dir / model_name
            exists = model_path.exists()
            status = "âœ…" if exists else "âŒ"
            
            if exists:
                size_mb = model_path.stat().st_size / (1024 * 1024)
                print(f"   {status} {model_name} ({size_mb:.1f} MB)")
            else:
                print(f"   {status} {model_name}")
                all_exist = False
    else:
        print("   âŒ LLAVA ç›®å½•ä¸å­˜åœ¨")
        all_exist = False
    
    return {"status": "success" if all_exist else "partial", "all_models_exist": all_exist}


@app.local_entrypoint()
def main():
    """æœ¬åœ°å…¥å£ç‚¹"""
    print("ğŸš€ éªŒè¯ LLAVA æ¨¡å‹...")
    result = verify_llava_models.remote()
    print(f"\nğŸ“Š éªŒè¯ç»“æœ: {result}")

