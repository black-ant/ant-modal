"""
ä¼ä¸šçŸ¥è¯†åº“æ™ºèƒ½æ£€ç´¢
ä¸šåŠ¡åœºæ™¯ï¼šä¼ä¸šç§¯ç´¯äº†å¤§é‡æ–‡æ¡£ï¼Œä½†å‘˜å·¥éš¾ä»¥å¿«é€Ÿæ‰¾åˆ°éœ€è¦çš„ä¿¡æ¯

è§£å†³çš„é—®é¢˜ï¼š
- ä¼ ç»Ÿå…³é”®è¯æœç´¢æ‰¾ä¸åˆ°è¯­ä¹‰ç›¸å…³çš„å†…å®¹
- å‘˜å·¥èŠ±å¤§é‡æ—¶é—´ç¿»æ‰¾æ–‡æ¡£ï¼Œæ•ˆç‡ä½ä¸‹
- æ–°äºº onboarding éœ€è¦é¢‘ç¹é—®åŒäº‹åŸºç¡€é—®é¢˜

è¿™ä¸ªä¾‹å­å±•ç¤ºï¼š
- æ–‡æ¡£å‘é‡åŒ–å­˜å‚¨
- è¯­ä¹‰æœç´¢ï¼ˆç†è§£é—®é¢˜å«ä¹‰ï¼‰
- ç›¸å…³æ–‡æ¡£æ¨è
- æ”¯æŒå¢é‡æ›´æ–°æ–‡æ¡£åº“
"""
import modal
import json
from datetime import datetime

app = modal.App("embedding-knowledge-base")

image = (
    modal.Image.debian_slim(python_version="3.11")
    .pip_install(
        "sentence-transformers",
        "torch==2.1.0",
        "numpy",
        "scikit-learn",
    )
)

model_volume = modal.Volume.from_name("embedding-models", create_if_missing=True)
kb_volume = modal.Volume.from_name("knowledge-base", create_if_missing=True)


# ç¤ºä¾‹çŸ¥è¯†åº“æ–‡æ¡£ï¼ˆå®é™…åœºæ™¯ä»æ•°æ®åº“/æ–‡ä»¶ç³»ç»ŸåŠ è½½ï¼‰
SAMPLE_DOCUMENTS = [
    {
        "id": "doc_001",
        "title": "å‘˜å·¥è¯·å‡æµç¨‹",
        "content": "å‘˜å·¥è¯·å‡éœ€è¦æå‰åœ¨ OA ç³»ç»Ÿæäº¤ç”³è¯·ï¼Œ1-3å¤©ç”±ç›´å±é¢†å¯¼å®¡æ‰¹ï¼Œ3å¤©ä»¥ä¸Šéœ€è¦éƒ¨é—¨æ€»ç›‘å®¡æ‰¹ã€‚ç—…å‡éœ€è¦æä¾›åŒ»é™¢è¯æ˜ã€‚",
        "category": "äººäº‹åˆ¶åº¦"
    },
    {
        "id": "doc_002",
        "title": "æŠ¥é”€åˆ¶åº¦è¯´æ˜",
        "content": "å·®æ—…æŠ¥é”€éœ€è¦åœ¨å‡ºå·®ç»“æŸå7å¤©å†…æäº¤ï¼Œéœ€è¦æä¾›å‘ç¥¨ã€è¡Œç¨‹å•ç­‰å‡­è¯ã€‚ä½å®¿æ ‡å‡†ï¼šä¸€çº¿åŸå¸‚500å…ƒ/æ™šï¼Œå…¶ä»–åŸå¸‚300å…ƒ/æ™šã€‚",
        "category": "è´¢åŠ¡åˆ¶åº¦"
    },
    {
        "id": "doc_003",
        "title": "ä»£ç å®¡æŸ¥è§„èŒƒ",
        "content": "æ‰€æœ‰ä»£ç æäº¤å‰å¿…é¡»ç»è¿‡ Code Reviewã€‚PR éœ€è¦è‡³å°‘ä¸€ä½åŒäº‹å®¡æ‰¹ã€‚å®¡æŸ¥é‡ç‚¹åŒ…æ‹¬ï¼šä»£ç é£æ ¼ã€é€»è¾‘æ­£ç¡®æ€§ã€æ€§èƒ½å½±å“ã€å®‰å…¨éšæ‚£ã€‚",
        "category": "ç ”å‘è§„èŒƒ"
    },
    {
        "id": "doc_004",
        "title": "ä¼šè®®å®¤é¢„çº¦æŒ‡å—",
        "content": "ä¼šè®®å®¤é€šè¿‡ä¼ä¸šå¾®ä¿¡æ—¥å†é¢„çº¦ã€‚å¤§ä¼šè®®å®¤ï¼ˆ10äººä»¥ä¸Šï¼‰éœ€è¦æå‰1å¤©é¢„çº¦ã€‚é¢„çº¦åæœªä½¿ç”¨ä¼šè¢«è®°å½•ï¼Œå½±å“åç»­é¢„çº¦æƒé™ã€‚",
        "category": "è¡Œæ”¿ç®¡ç†"
    },
    {
        "id": "doc_005",
        "title": "æ–°äººå…¥èŒæŒ‡å—",
        "content": "å…¥èŒç¬¬ä¸€å¤©éœ€è¦åˆ° HR å¤„é¢†å–å·¥å¡ã€ç”µè„‘ç­‰åŠå…¬ç”¨å“ã€‚ç¬¬ä¸€å‘¨éœ€è¦å®Œæˆï¼šä¼ä¸šæ–‡åŒ–åŸ¹è®­ã€éƒ¨é—¨ä»‹ç»ã€å¯¼å¸ˆ 1v1ã€ç³»ç»Ÿæƒé™å¼€é€šã€‚",
        "category": "äººäº‹åˆ¶åº¦"
    },
    {
        "id": "doc_006",
        "title": "VPN ä½¿ç”¨è¯´æ˜",
        "content": "è¿œç¨‹åŠå…¬éœ€è¦ä½¿ç”¨ VPN è¿æ¥å…¬å¸ç½‘ç»œã€‚ä¸‹è½½åœ°å€ï¼šå†…ç½‘ IT æœåŠ¡é¡µé¢ã€‚é¦–æ¬¡ä½¿ç”¨éœ€è¦ç”³è¯· VPN è´¦å·ï¼Œå®¡æ‰¹å IT ä¼šå‘é€é…ç½®ä¿¡æ¯ã€‚",
        "category": "ITæ”¯æŒ"
    },
    {
        "id": "doc_007",
        "title": "å¹´å‡æ”¿ç­–",
        "content": "å‘˜å·¥å…¥èŒæ»¡ä¸€å¹´åäº«æœ‰5å¤©å¸¦è–ªå¹´å‡ã€‚å·¥é¾„æ¯å¢åŠ ä¸€å¹´å¢åŠ 1å¤©ï¼Œä¸Šé™15å¤©ã€‚å¹´å‡å¯ä»¥ç´¯ç§¯åˆ°æ¬¡å¹´3æœˆåº•ï¼Œè¿‡æœŸä½œåºŸã€‚",
        "category": "äººäº‹åˆ¶åº¦"
    },
    {
        "id": "doc_008",
        "title": "é¡¹ç›®ç«‹é¡¹æµç¨‹",
        "content": "æ–°é¡¹ç›®éœ€è¦å¡«å†™ç«‹é¡¹ç”³è¯·ä¹¦ï¼ŒåŒ…æ‹¬é¡¹ç›®èƒŒæ™¯ã€ç›®æ ‡ã€èµ„æºéœ€æ±‚ã€æ—¶é—´è®¡åˆ’ã€‚ç»æŠ€æœ¯è¯„å®¡ä¼šå’Œä¸šåŠ¡è¯„å®¡ä¼šé€šè¿‡åæ­£å¼ç«‹é¡¹ã€‚",
        "category": "é¡¹ç›®ç®¡ç†"
    },
]


@app.cls(
    image=image,
    gpu="T4",
    volumes={"/models": model_volume, "/kb": kb_volume},
    timeout=600,
)
class KnowledgeBase:
    @modal.enter()
    def load_model(self):
        from sentence_transformers import SentenceTransformer
        import numpy as np
        import os
        
        print("ğŸ”¤ åŠ è½½åµŒå…¥æ¨¡å‹...")
        self.model = SentenceTransformer(
            "sentence-transformers/paraphrase-multilingual-mpnet-base-v2",
            cache_folder="/models"
        )
        
        # å°è¯•åŠ è½½å·²æœ‰çš„æ–‡æ¡£ç´¢å¼•
        self.documents = []
        self.embeddings = None
        
        index_path = "/kb/index.json"
        embeddings_path = "/kb/embeddings.npy"
        
        if os.path.exists(index_path) and os.path.exists(embeddings_path):
            print("ğŸ“š åŠ è½½å·²æœ‰çŸ¥è¯†åº“ç´¢å¼•...")
            with open(index_path, "r", encoding="utf-8") as f:
                self.documents = json.load(f)
            self.embeddings = np.load(embeddings_path)
            print(f"âœ“ å·²åŠ è½½ {len(self.documents)} ä¸ªæ–‡æ¡£")
        else:
            print("ğŸ“ çŸ¥è¯†åº“ä¸ºç©ºï¼Œéœ€è¦å…ˆå¯¼å…¥æ–‡æ¡£")
        
        print("âœ“ åˆå§‹åŒ–å®Œæˆ")
    
    @modal.method()
    def index_documents(self, documents: list[dict]) -> dict:
        """
        ç´¢å¼•æ–‡æ¡£åˆ°çŸ¥è¯†åº“
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨ [{"id": "...", "title": "...", "content": "...", "category": "..."}]
        """
        import numpy as np
        
        print(f"ğŸ“š ç´¢å¼• {len(documents)} ä¸ªæ–‡æ¡£...")
        
        # åˆå¹¶ç°æœ‰æ–‡æ¡£
        existing_ids = {doc["id"] for doc in self.documents}
        new_docs = [d for d in documents if d["id"] not in existing_ids]
        
        if not new_docs:
            return {"status": "no_new_documents", "total": len(self.documents)}
        
        # ç”Ÿæˆæ–°æ–‡æ¡£çš„åµŒå…¥
        texts = [f"{d['title']}. {d['content']}" for d in new_docs]
        new_embeddings = self.model.encode(texts, convert_to_numpy=True)
        
        # åˆå¹¶
        self.documents.extend(new_docs)
        
        if self.embeddings is not None:
            self.embeddings = np.vstack([self.embeddings, new_embeddings])
        else:
            self.embeddings = new_embeddings
        
        # ä¿å­˜åˆ° Volume
        with open("/kb/index.json", "w", encoding="utf-8") as f:
            json.dump(self.documents, f, ensure_ascii=False, indent=2)
        np.save("/kb/embeddings.npy", self.embeddings)
        
        kb_volume.commit()
        
        print(f"âœ“ æ–°å¢ {len(new_docs)} ä¸ªæ–‡æ¡£ï¼Œæ€»è®¡ {len(self.documents)} ä¸ª")
        
        return {
            "status": "success",
            "new_documents": len(new_docs),
            "total_documents": len(self.documents)
        }
    
    @modal.method()
    def search(
        self,
        query: str,
        top_k: int = 5,
        category: str = None,
        min_score: float = 0.3
    ) -> list[dict]:
        """
        è¯­ä¹‰æœç´¢çŸ¥è¯†åº“
        
        Args:
            query: æœç´¢é—®é¢˜
            top_k: è¿”å›å‰ k ä¸ªç»“æœ
            category: é™å®šåˆ†ç±»ï¼ˆå¯é€‰ï¼‰
            min_score: æœ€å°ç›¸ä¼¼åº¦é˜ˆå€¼
        """
        from sklearn.metrics.pairwise import cosine_similarity
        import numpy as np
        
        if not self.documents or self.embeddings is None:
            return []
        
        print(f"ğŸ” æœç´¢: {query}")
        
        # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
        query_embedding = self.model.encode([query])[0]
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = cosine_similarity(
            query_embedding.reshape(1, -1),
            self.embeddings
        )[0]
        
        # ç­›é€‰å’Œæ’åº
        results = []
        for i, score in enumerate(similarities):
            doc = self.documents[i]
            
            # åˆ†ç±»è¿‡æ»¤
            if category and doc.get("category") != category:
                continue
            
            # åˆ†æ•°è¿‡æ»¤
            if score < min_score:
                continue
            
            results.append({
                "id": doc["id"],
                "title": doc["title"],
                "content": doc["content"],
                "category": doc.get("category", ""),
                "score": float(score)
            })
        
        # æ’åºå¹¶è¿”å› top_k
        results.sort(key=lambda x: x["score"], reverse=True)
        
        return results[:top_k]
    
    @modal.method()
    def get_related_documents(self, doc_id: str, top_k: int = 3) -> list[dict]:
        """
        è·å–ç›¸å…³æ–‡æ¡£æ¨è
        """
        from sklearn.metrics.pairwise import cosine_similarity
        
        # æ‰¾åˆ°ç›®æ ‡æ–‡æ¡£
        target_idx = None
        for i, doc in enumerate(self.documents):
            if doc["id"] == doc_id:
                target_idx = i
                break
        
        if target_idx is None:
            return []
        
        # è®¡ç®—ä¸å…¶ä»–æ–‡æ¡£çš„ç›¸ä¼¼åº¦
        similarities = cosine_similarity(
            self.embeddings[target_idx].reshape(1, -1),
            self.embeddings
        )[0]
        
        # æ’åºï¼ˆæ’é™¤è‡ªèº«ï¼‰
        results = []
        for i, score in enumerate(similarities):
            if i == target_idx:
                continue
            results.append({
                "id": self.documents[i]["id"],
                "title": self.documents[i]["title"],
                "category": self.documents[i].get("category", ""),
                "score": float(score)
            })
        
        results.sort(key=lambda x: x["score"], reverse=True)
        return results[:top_k]


@app.function(image=image)
@modal.web_endpoint(method="POST")
def kb_search_api(data: dict):
    """
    çŸ¥è¯†åº“æœç´¢ API
    
    POST /kb_search_api
    {
        "query": "å¦‚ä½•ç”³è¯·å¹´å‡ï¼Ÿ",
        "top_k": 5,
        "category": "äººäº‹åˆ¶åº¦"  // å¯é€‰
    }
    """
    kb = KnowledgeBase()
    
    results = kb.search.remote(
        query=data.get("query", ""),
        top_k=data.get("top_k", 5),
        category=data.get("category")
    )
    
    return {
        "status": "success",
        "query": data.get("query"),
        "results": results
    }


@app.function(image=image)
@modal.web_endpoint(method="POST")
def kb_index_api(data: dict):
    """
    ç´¢å¼•æ–‡æ¡£ API
    
    POST /kb_index_api
    {
        "documents": [
            {"id": "doc_new", "title": "...", "content": "...", "category": "..."}
        ]
    }
    """
    kb = KnowledgeBase()
    result = kb.index_documents.remote(data.get("documents", []))
    return result


@app.local_entrypoint()
def main():
    """æ¼”ç¤ºçŸ¥è¯†åº“æ£€ç´¢"""
    print("ğŸ“š ä¼ä¸šçŸ¥è¯†åº“æ™ºèƒ½æ£€ç´¢")
    print("=" * 50)
    
    kb = KnowledgeBase()
    
    # 1. ç´¢å¼•ç¤ºä¾‹æ–‡æ¡£
    print("\n1ï¸âƒ£ ç´¢å¼•ç¤ºä¾‹æ–‡æ¡£...")
    result = kb.index_documents.remote(SAMPLE_DOCUMENTS)
    print(f"   ç»“æœ: {result}")
    
    # 2. æµ‹è¯•è¯­ä¹‰æœç´¢
    test_queries = [
        "æˆ‘æƒ³è¯·å‡ å¤©å‡ï¼Œéœ€è¦èµ°ä»€ä¹ˆæµç¨‹ï¼Ÿ",
        "å‡ºå·®ä½é…’åº—æœ‰ä»€ä¹ˆæ ‡å‡†ï¼Ÿ",
        "æ–°åŒäº‹åˆšæ¥å…¬å¸è¦åšä»€ä¹ˆï¼Ÿ",
        "æ€ä¹ˆåœ¨å®¶è¿œç¨‹åŠå…¬ï¼Ÿ",
    ]
    
    print("\n2ï¸âƒ£ æµ‹è¯•è¯­ä¹‰æœç´¢:")
    
    for query in test_queries:
        print(f"\nâ“ é—®é¢˜: {query}")
        results = kb.search.remote(query, top_k=2)
        
        for i, r in enumerate(results, 1):
            print(f"   {i}. [{r['score']:.3f}] {r['title']}")
            print(f"      {r['content'][:60]}...")
    
    # 3. æµ‹è¯•ç›¸å…³æ–‡æ¡£æ¨è
    print("\n3ï¸âƒ£ ç›¸å…³æ–‡æ¡£æ¨è:")
    related = kb.get_related_documents.remote("doc_001", top_k=3)
    print("   ä¸ã€Œå‘˜å·¥è¯·å‡æµç¨‹ã€ç›¸å…³çš„æ–‡æ¡£:")
    for r in related:
        print(f"   - [{r['score']:.3f}] {r['title']}")
    
    print("\nğŸ’¡ æç¤º:")
    print("1. å®é™…ä½¿ç”¨æ—¶ä»æ•°æ®åº“/æ–‡ä»¶ç³»ç»ŸåŠ è½½æ–‡æ¡£")
    print("2. æ”¯æŒå¢é‡æ›´æ–°ï¼Œæ— éœ€å…¨é‡é‡å»ºç´¢å¼•")
    print("3. å¯é…åˆ LLM å®ç°é—®ç­”å¼çŸ¥è¯†åº“")

